{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 7: Bayes Rule Rules\n",
    "\n",
    "In this workshop, we'll be looking at how to use Naive Bayes and Bayes Nets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "# set a seed for reproducibility\n",
    "random_seed = 25\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Naive Bayes Spam Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One historical use of Naive Bayes is to try and detect [spam emails](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering). \n",
    "\n",
    "In this exercise, you will be using dataset that of emails from the [Enron Corporation](https://en.wikipedia.org/wiki/Enron_Corpus), an accounting firm that [went bankrupt in 2001 due to an accounting scandal](https://en.wikipedia.org/wiki/Enron_scandal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Exploring the Data (Follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./enron_emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\nth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\n( see a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>Subject: neon retreat\\nho ho ho , we ' re arou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>Subject: re : indian springs\\nthis deal is to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>Subject: put the 10 on the ft\\nthe transport v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>Subject: 3 / 4 / 2000 and following noms\\nhpl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>Subject: calpine daily gas nomination\\n&gt;\\n&gt;\\nj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>Subject: industrial worksheets for august 2000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "      <td>Subject: important online banking alert\\ndear ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5171 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  label_num                                               text\n",
       "0      ham          0  Subject: enron methanol ; meter # : 988291\\nth...\n",
       "1      ham          0  Subject: hpl nom for january 9 , 2001\\n( see a...\n",
       "2      ham          0  Subject: neon retreat\\nho ho ho , we ' re arou...\n",
       "3     spam          1  Subject: photoshop , windows , office . cheap ...\n",
       "4      ham          0  Subject: re : indian springs\\nthis deal is to ...\n",
       "...    ...        ...                                                ...\n",
       "5166   ham          0  Subject: put the 10 on the ft\\nthe transport v...\n",
       "5167   ham          0  Subject: 3 / 4 / 2000 and following noms\\nhpl ...\n",
       "5168   ham          0  Subject: calpine daily gas nomination\\n>\\n>\\nj...\n",
       "5169   ham          0  Subject: industrial worksheets for august 2000...\n",
       "5170  spam          1  Subject: important online banking alert\\ndear ...\n",
       "\n",
       "[5171 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping with the theme of meat products, some researchers call emails that are *not spam*, **ham**. \n",
    "\n",
    "To sum up: a **ham** email is a legitimate email, while a **spam** email is unwanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     3672\n",
       "spam    1499\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at our distriubtion of spam and ham emails\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some of the ham emails..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: ehronline web address change\n",
      "this message is intended for ehronline users only .\n",
      "due to a recent change to ehronline , the url ( aka \" web address \" ) for accessing ehronline needs to be changed on your computer . the change involves adding the letter \" s \" to the \" http \" reference in the url . the url for accessing ehronline should be : https : / / ehronline . enron . com .\n",
      "this change should be made by those who have added the url as a favorite on the browser .\n"
     ]
    }
   ],
   "source": [
    "print(df[df[\"label\"]==\"ham\"].text.iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the spam emails..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: underpriced issue with high return on equity\n",
      "stock report .\n",
      "dont sieep on this stock ! this is a hot one !\n",
      "company : gaming transactions inc .\n",
      "stock symbol : ggts\n",
      "currentiy trading at : o . 30\n",
      "rating : strong purchase\n",
      "near - term target : 0 . 45\n",
      "long - term target : 1 . oo\n",
      "breaking news for ggts :\n",
      "gaming transactions inc . ( ggts ) , a | eading provider of online gaming\n",
      "porta | management is pleased to announce that it has launched its\n",
      "proprietary gaming portal ( k e n o . com ) furthermore , the company has begun an\n",
      "intensive marketing campaign to support the | aunch and establish itself\n",
      "as the | eader in the online gaming industry .\n",
      "( k e n o . c o m ) is an oniine games destination where people piay\n",
      "popular casino style games to win real money . the foundation of the site is\n",
      "an online version of keno . the game of keno uses 80 balls numbered 1\n",
      "thru 8 o . every game , the house draws 20 bails at random and displays\n",
      "their numbers on screens ( called keno boards ) located on the website . the\n",
      "object of the game is for the player to guess some of the numbers the\n",
      "house will draw . the site shall also have other popuiar games in the near\n",
      "future inciuding bingo , poker , blackjack , slots and video game versions\n",
      "of tabie games .\n",
      "patrick smyth , ceo of gaming transactions inc . , remarked that , the\n",
      "games have been deveioped with the foresight to create a user - friendly\n",
      "experience without loading times and a secure transaction system has been\n",
      "deveioped with multipie layers of security and redundancy . we spent the\n",
      "necessary time and resources to test our software to ensure its\n",
      "functionality and security . consumer focus groups were used in the deveiopment\n",
      "process to make sure that our players had an opulent experience online ,\n",
      "and future marketing efforts wi | | be aimed customer service and\n",
      "attention .\n",
      "about the company :\n",
      "gaming transactions inc . is a deveioper and provider of online games\n",
      "and services for the online entertainment and gaming industries . the\n",
      "company ! s centra | licensed games portal , ( k e n o . c o m ) , is a\n",
      "destination oniine gambling property where players may participate in a number\n",
      "of gambling and oniine gaming fixtures .\n",
      "the foundation of the site is of course an online version of keno . the\n",
      "game of keno uses 8 o bails numbered 1 thru 80 . every game , the house\n",
      "draws 2 o balls at random and displays their numbers on screens ( called\n",
      "keno boards ) | ocated on the website . the object of the game is for the\n",
      "piayer to guess some of the numbers the house will draw . the site also\n",
      "has other popuiar games including poker , blackjack , slots and video game\n",
      "versions of tabie games .\n",
      "gaming transactions inc . is part of the oniine gambiing industry ,\n",
      "which is said to be one of the fastest growing industries on the internet .\n",
      "! ' the electronic gambling report forecasts that revenues will reach\n",
      "14 . 5 biliion by 2006 ! ( market statistics : - informa media grp . giobal\n",
      "revenues from oniine gambiing wi | | reach 14 . 52 bi | | ion in 20 o 6 , up from\n",
      "3 . 81 biilion this year . this is according to a report from the informa\n",
      "media grp . , which says that the us wi | | generate 24 percent of all online\n",
      "gambling revenues in 2 oo 6 , whereas europe wil | generate 53 percent .\n",
      "north american oniine gambling revenues are expected to reach 1 . 99 biliion\n",
      "this year and 3 . 85 bi | | ion in 2 oo 6 . in europe , revenues wil | grow from\n",
      "1 . 29 billion this year to 7 . 64 biilion in 2 oo 6 . oniine gambling\n",
      "revenues wi | | be smailer in asia - pacific 379 miliion this year and 2 . 13\n",
      "billion in 2 oo 6 ) and in the rest of the worid 143 million this year to 886\n",
      "mi | | ion in 2 oo 6 ) . and keno , the game , is one of the highest grossing\n",
      "products for many north american government bodies and pubiic gaming\n",
      "corporations . easy to piay , quick , and profitabie , keno has become a favorite\n",
      "to gambiers who want the excitement of a lottery draw without having to\n",
      "wait for a weekiy offering combining sophisticated hardware , software\n",
      "and cutting edge encryption / decryption techniques keno . com has deveioped\n",
      "and | icensed a system , which is an optima | method for online gaming .\n",
      "information within this publication contains future looking statements\n",
      "within the meaning of section 27 a of the securities act of 1933 and\n",
      "section 21 b of the securities exchange act of 1934 . any statements that\n",
      "express or involve discussions with respect to predictions ,\n",
      "expectations , beliefs , pians , projections , objectives , goals , assumptions or future\n",
      "events or performance are not statements of historica | fact and may be\n",
      "future looking statements . future | ooking statements are based on\n",
      "expectations , estimates and projections at the time the statements are made\n",
      "that involve a number of risks and uncertainties which couid cause\n",
      "actual results or events to differ materia | | y from those presently\n",
      "anticipated . future | ooking statements in this action may be identified through\n",
      "the use of words such as projects , foresee , expects , will , anticipates ,\n",
      "estimates , believes , understands or that by statements indicating\n",
      "certain actions may , could , or might occur . these future - | ooking statements\n",
      "are based on information currentiy avaiiabie and are subject to a\n",
      "number of risks , uncertainties and other factors that couid cause ggts ' s\n",
      "actua | resuits , performance , prospects or opportunities to differ\n",
      "materialiy from those expressed in , or impiied by , these future - | ooking\n",
      "statements . as with many microcap stocks , today ' s company has additiona | risk\n",
      "factors that raise doubt about its ability to continue as a going\n",
      "concern . ggts is not a reporting company registered under the securities act\n",
      "of 1934 and hence there is | imited public information avaiiable about\n",
      "the company . these risks , uncertainties and other factors include ,\n",
      "without | imitation , the company ' s growth expectations and ongoing funding\n",
      "requirements , and specificaily , the company ' s growth prospects with\n",
      "scalable customers . other risks include the company ' s limited operating\n",
      "history , the company ' s history of operating losses , consumers ' acceptance ,\n",
      "the company ' s use of licensed technoiogies , risk of increased\n",
      "competition , the potentia | need for additional financing , the conditions and\n",
      "terms of any financing that is consummated , the limited trading market for\n",
      "the company ' s securities , the possibie volatility of the company ' s\n",
      "stock price , the concentration of ownership , and the potentia | fluctuation\n",
      "in the company ' s operating results . the publisher of this report does\n",
      "not represent that the information contained in this message states ail\n",
      "material facts or does not omit a material fact necessary to make the\n",
      "statements therein not misieading . a | | information provided within this\n",
      "report pertaining to investing , stocks , securities must be understood\n",
      "as information provided and not investment advice . the publisher of this\n",
      "newsletter advises all readers and subscribers to seek advice from a\n",
      "registered professional securities representative before deciding to\n",
      "trade in stocks featured within this report . none of the materia | within\n",
      "this report shall be construed as any kind of investment advice or\n",
      "solicitation . many of these companies are on the verge of bankruptcy . you can\n",
      "lose al | your money by investing in this stock . the pubiisher of this\n",
      "report is not a registered investment expert . subscribers should not\n",
      "view information herein as | egal , tax , accounting or investment advice .\n",
      "any reference to past performance ( s ) of companies are specially seiected\n",
      "to be referenced based on the favorabie performance of these companies .\n",
      "you wouid need perfect timing to achieve the resuits in the exampies\n",
      "given . there can be no assurance of that happening . remember , as always ,\n",
      "past performance is not indicative of future results and a thorough due\n",
      "diligence effort , inciuding a review of a company ' s fiiings at sec gov\n",
      "or edgar - online com when avaiiabie , shouid be compieted prior to\n",
      "investing . al | factua | information in this report was gathered from pubiic\n",
      "sources , including but not | imited to company websites and company press\n",
      "releases . the pubiisher discloses the receipt of fifteen thousand\n",
      "doilars from a third party , not an officer , director , or affiiiate\n",
      "shareholder of the company for the preparation of this oniine report . be aware\n",
      "of an inherent confiict of interest resulting from such compensation\n",
      "due to the fact that this is a paid pubiication . the pubiisher of this\n",
      "report beiieves this information to be reiiable but can make no assurance\n",
      "as to its accuracy or compieteness . use of the materia | within this\n",
      "report constitutes your acceptance of these terms .\n",
      "if you wish to stop future maiiings , or if you feel you have been\n",
      "wrongfuliy placed in our membership , piease go here or send a biank\n",
      "e mai | with no thanks in the subject to ( - stoxo 042 @ yahoo . com - )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[df[\"label\"]==\"spam\"].text.iloc[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try exploring different emails by changing the index in the lines above. **What common traits do you notice accross the ham emails? The spam emails?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Bag of Words (Follow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we used tf-idf to represent words as feature vectors. However, sometimes simpler methods work just as well (if not better). For this, we'll be using the [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation of a piece of text, which is much more interpretable than tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer's `fit_transform` method returns a NxM matrix. `N` is the number of documents (sentences) you have in your corpus, and `M` is the number of unique words in your corpus. Item `n`x`m` is how many times word `m` appears in document `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more interpretable view..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first document.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 1,\n",
       " 'is': 1,\n",
       " 'one': 0,\n",
       " 'second': 0,\n",
       " 'the': 1,\n",
       " 'third': 0,\n",
       " 'this': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(corpus[0])\n",
    "dict(zip(vectorizer.get_feature_names(),X.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is the second document.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 2,\n",
       " 'first': 0,\n",
       " 'is': 1,\n",
       " 'one': 0,\n",
       " 'second': 1,\n",
       " 'the': 1,\n",
       " 'third': 0,\n",
       " 'this': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(corpus[1])\n",
    "dict(zip(vectorizer.get_feature_names(),X.toarray()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you want to vectorize new data (e.g. test data), then you use the `.transform` function. If the vectorizer encounters a word it hasn't seen before, it will simply ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([\"This is the coolest document\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3) Building and Running the Model (Group)\n",
    "\n",
    "Now that you have all the required tools, build a **Naive Bayes Classifier** and evaluate it on a train and test set. In this instance, [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) classifier, which is most useful for discrete features that use frequency counts (e.g. a bag of words vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test splits - 20% split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize on your training data using BoW\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the classifier below\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X,train.label_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize your test data using transform and then predict the test data\n",
    "test_vecs = vectorizer.transform(test.text)\n",
    "predictions = clf.predict(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[728,   6],\n",
       "       [ 11, 290]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a confusion matrix using confusion_matrix\n",
    "confusion_matrix(test.label_num,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# test.label_num\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       734\n",
      "          1       0.98      0.96      0.97       301\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report using classification_report()\n",
    "print(classification_report(test.label_num,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Exploring Important Words (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, predict what words might be more predictive of SPAM or HAM. Make a list below of 5 words you think will be very _predictive_ of an email being SPAM, and 5 words that are predictive of being HAM. Remember this is an office email database from Enron in the 1990s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words the will predict SPAM (junk emails):\n",
    "\n",
    "1. x\n",
    "2. x\n",
    "3. x\n",
    "4. x\n",
    "5. x\n",
    "\n",
    "Words the will predict HAM (real emails):\n",
    "\n",
    "1. x\n",
    "2. x\n",
    "3. x\n",
    "4. x\n",
    "5. x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical Note: Log Probabilities**: \n",
    "\n",
    "When using probabilistic methods with large datasets, sometimes you get features with extremely small probabilities (e.g. $10^{-10}$). \n",
    "\n",
    "This becomes a problem, because computers aren't really good at doing operations with numbers at this scale. Therefore, in most systems, operations are done on the *log* of the probabilities. \n",
    "\n",
    "This makes calculations much more managable (e.g. $\\log(10^{-10})=-10$). As an added bonus, due to log rules ($log(ab)=log(a)+log(b)$), all multiplications turn into additions, which are easier for the computer.\n",
    "\n",
    "Some general rules of thumb: **the closer to zero a log prob is, the more probabable it is**, and **each time a log prob decreases by one, it's an order of magnitude less probable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature_log_probs` gives us the log probabilities for each word. In notation, each of these are $P(word | class)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.75023447,  -5.71398455, -11.28296498, ..., -13.07472445,\n",
       "       -13.07472445, -13.07472445])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given that a message is ham, how probable is it for the words to show up?\n",
    "clf.feature_log_prob_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.23363088,  -7.07284789,  -9.8691907 , ..., -11.74099287,\n",
       "       -11.74099287, -11.74099287])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given that a message is SPAM, how probable is it for the words to show up?\n",
    "clf.feature_log_prob_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will sort all the words by log probability, so that all of the most probable words show up first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flip() missing 1 required positional argument: 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d7fe54e0a4bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mspam_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_log_prob_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mspam_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mspam_args\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mspam_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspam_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mham_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_log_prob_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: flip() missing 1 required positional argument: 'axis'"
     ]
    }
   ],
   "source": [
    "spam_args = np.argsort(clf.feature_log_prob_[1])\n",
    "spam_words = np.array(vectorizer.get_feature_names())[spam_args]\n",
    "spam_words = np.flip(spam_words)\n",
    "\n",
    "ham_args = np.argsort(clf.feature_log_prob_[0])\n",
    "ham_words = np.array(vectorizer.get_feature_names_out())[ham_args]\n",
    "ham_words = np.flip(spam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cima', 'breveffo', 'brewer', 'skytel', 'skymiles', 'firmed',\n",
       "       'skydivespaceland', 'skydive', 'skutchin', 'briana'], \n",
       "      dtype='<U24')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ham_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b3426b3fc4b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mham_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ham_words' is not defined"
     ]
    }
   ],
   "source": [
    "ham_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a more useful way to look at the data is to look at the *ratios* of the probabilities for a given word. For example, if we have the word \"free\":\n",
    "\n",
    "*If an email is spam, there is a 50% probability it will contain the word \"free\"*\n",
    "\n",
    "$P(free|spam)=0.5$\n",
    "\n",
    "*If an email is ham, there is a 10% probability it will contain the word \"free\"*\n",
    "\n",
    "$P(free|ham)=0.1$\n",
    "\n",
    "*The ratio*\n",
    "\n",
    "$P(free|spam)/P(free|ham)=5$\n",
    "\n",
    "This means that the word *free* is 5x as more likely to show up in spam messages compared to ham messages. So, we can use this to calculate and sort for words that are proportionally more present in spam emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flip() missing 1 required positional argument: 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-bf46c49bdd11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mspam_ham_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_odds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mspam_ham_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mspam_ham_args\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mspam_ham_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspam_ham_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: flip() missing 1 required positional argument: 'axis'"
     ]
    }
   ],
   "source": [
    "# Since we're operating on logs, division turns into subtraction\n",
    "log_odds = clf.feature_log_prob_[1] - clf.feature_log_prob_[0]\n",
    "spam_ham_args = np.argsort(log_odds)\n",
    "spam_ham_words = np.array(vectorizer.get_feature_names())[spam_ham_args]\n",
    "spam_ham_words = np.flip(spam_ham_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some of the \"spammiest\" words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['enron', 'ect', 'meter', 'hpl', 'daren', 'mmbtu', 'xls', 'pec',\n",
       "       'sitara', 'hou', 'volumes', 'ena', 'forwarded', 'melissa',\n",
       "       'tenaska', 'teco', 'nom', '2001', 'pat', 'aimee', 'actuals', 'noms',\n",
       "       'hsc', 'susan', 'cotten', 'chokshi', 'nomination', 'fyi',\n",
       "       'pipeline', 'wellhead', 'eastrans', 'clynes', 'hplc', '713',\n",
       "       'counterparty', 'pefs', 'bob', 'nominations', 'cec', 'gcs',\n",
       "       'lannou', 'txu', 'farmer', 'hplno', 'rita', 'weissman', 'cc',\n",
       "       'equistar', 'enronxgate', 'iferc', 'scheduled', 'spreadsheet',\n",
       "       'wynne', 'allocated', 'entex', 'path', 'buyback', 'fuels', 'hplo',\n",
       "       'lisa', 'scheduling', 'pops', 'anita', 'calpine', 'gco', 'darren',\n",
       "       'clem', 'steve', 'aep', 'katy', 'tu', 'flowed', 'follows',\n",
       "       'sherlyn', 'donna', 'lloyd', 'midcon', 'pm', 'redeliveries',\n",
       "       'jackie', 'gary', 'vance', 'papayoti', 'meters', 'cornhusker',\n",
       "       'luong', 'howard', 'pg', 'lsk', 'revision', 'julie', 'utilities',\n",
       "       '281', 'bryan', 'dfarmer', 'ees', 'reinhardt', 'hplnl', 'cleburne',\n",
       "       'valero', 'unify', 'outage', 'poorman', 'victor', 'methanol',\n",
       "       '6353', 'tap', 'baumbach', 'devon', 'lsp', 'lamphier', 'herod',\n",
       "       'liz', 'schumack', 'enserch', 'employee', '098', 'boas', 'megan',\n",
       "       'meyers', 'allocation', 'deliveries', 'easttexas', 'ami',\n",
       "       'enrononline', 'invoice', 'withers', 'taylor', 'robert', 'bellamy',\n",
       "       'fred', 'gpgfin', 'avila', 'pathed', 'duke', 'spoke', 'mccoy',\n",
       "       'cernosek', 'oasis', 'carlos', 'kevin', '1266', 'saturday', '853',\n",
       "       'riley', 'tejas', 'waha', 'katherine', 'kcs', 'graves', 'logistics',\n",
       "       'revised', 'paso', '345', 'eileen', 'hakemack', 'mm', 'ponton',\n",
       "       'cdnow', 'hesco', 'cp', 'reliantenergy', 'sandi', 'btu', 'mckay',\n",
       "       'gomes', 'chad', '0435', 'superty', 'lamadrid', '4179', 'tisdale',\n",
       "       'neon', 'lauri', 'interconnect', 'aepin', 'neuweiler', 'herrera',\n",
       "       'attached', 'panenergy', 'acton', 'tess', 'deal', 'rodriguez',\n",
       "       'mops', 'holmes', 'coastal', 'imbalance', 'stacey',\n",
       "       'availabilities', 'eol', 'pinion', 'heidi', 'camp', 'brenda',\n",
       "       'mary', 'origination', 'charlene', 'billed', 'lee'], \n",
       "      dtype='<U24')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_x=200\n",
    "spam_ham_words[0:top_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that words like `td`, `nbsp` and `br` are all HTML tags (for tables, spaces and newlines, respectively. This suggests that SPAM is more likely to have fancy HTML formatting than HAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse the list, and now we have the \"hammiest\" words... (words most indicative of a legitimate email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flip() missing 1 required positional argument: 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-17143d030ec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspam_ham_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtop_x\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: flip() missing 1 required positional argument: 'axis'"
     ]
    }
   ],
   "source": [
    "np.flip(spam_ham_words)[0:top_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at the words that best distinguish SPAM and HAM:**\n",
    "1. How many of your words showed up in the SPAM and HAM top 200 predictive words?\n",
    "2. Are they what you would have expected?\n",
    "3. Based on this, what can you say about the differences between how people make prediction and ML algorithms make predictions?\n",
    "4. Does this make you more confident or less confident in ML predictions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Bayesian Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we'll be using the `ASIA` dataset, which showcases the reltionships between travel, smoking, etc. and the probabilty of having various conditions. We will be using the [pomegranate](https://pomegranate.readthedocs.io/en/latest/) library to handle the heavy lifting for Bayes Nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./asia_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Exploring Bayes Nets (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First go here: https://www.bayesserver.com/examples/networks/asia\n",
    "\n",
    "Try checking different boxes and seeing how the model updates. When you check a box, you're \"given\" a specific value for that node. For example, checking \"True\" for \"Visit to Asia\" means the patient has visited Asia, but we don't know the other probabilities yet. The new probabilities are \"given\" that you've visited Asia.\n",
    "\n",
    "Now answer the following questions. For each one, first make a **prediction** about how the model will change, and the try it to see if you're right. Write down your prediction and then the actual answer. If your prediction differs than the actual answer, try and discuss why.\n",
    "\n",
    "**After each question, uncheck all boxes.**\n",
    "\n",
    "1. If you set the value of Visit to Asia, which nodes will update?\n",
    "2. If you set the value of XRay Result, which nodes with update?\n",
    "3. First set the value for Has Tuberculosis. If you then set the value for Visit to Asia, which nodes will update?\n",
    "3. First set the value for Tuberculosis or Cancer. If you then set the value for Dyspnea, which nodes will update?\n",
    "4. If you check the box for Has Tuberculosis, will Has Lunch Cancer update?\n",
    "5. First set the value for Tuberculosis or Cancer. Now if you check the box for Has Tuberculosis, will Has Lunch Cancer update?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Building the Bayes Net (Follow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in class, we can define the initial structure and conditional probability tables for the Bayes Net using our expert knowledge of the scenario (in this case, given to use by experts).\n",
    "\n",
    "For example, the first table gives the probability of having TB give that you have (or have not) visited Asia.\n",
    "\n",
    "| Asia | HasTB | P(HasTB\\|Asia) |\n",
    "| ---- | ----- | ------------- |\n",
    "| T | T | 0.05 |\n",
    "| T | F | 0.95 |\n",
    "| F | T | 0.01 |\n",
    "| F | F | 0.99 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pomegranate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, we define our top level nodes with their base probabilities.\n",
    "\n",
    "visit_to_asia = DiscreteDistribution({'T':0.01, 'F':0.99})\n",
    "smoke = DiscreteDistribution({'T':0.5, 'F':0.5})\n",
    "\n",
    "# Now, we have to fill in all of the conditional probability tables for the other nodes\n",
    "\n",
    "has_tb = ConditionalProbabilityTable(\n",
    "    [\n",
    "        #Asia? #HasTB #Probability\n",
    "        [\"T\",\"T\",0.05],\n",
    "        [\"T\",\"F\",0.95],\n",
    "        \n",
    "        [\"F\",\"T\",0.01],\n",
    "        [\"F\",\"F\",0.99],\n",
    "    ], [visit_to_asia])\n",
    "\n",
    "\n",
    "has_lung_cancer = ConditionalProbabilityTable(\n",
    "    [\n",
    "        #Smoke? \n",
    "        [\"T\",\"T\",0.1],\n",
    "        [\"T\",\"F\",0.9],\n",
    "        \n",
    "        [\"F\",\"T\",0.01],\n",
    "        [\"F\",\"F\",0.99]\n",
    "    ], [smoke])\n",
    "\n",
    "\n",
    "has_bc = ConditionalProbabilityTable(\n",
    "    [\n",
    "        #Smoke?\n",
    "        [\"T\",\"T\",0.6],\n",
    "        [\"T\",\"F\",0.4],\n",
    "        \n",
    "        [\"F\",\"T\",0.3],\n",
    "        [\"F\",\"F\",0.7]\n",
    "    ], [smoke])\n",
    "\n",
    "tb_or_cancer = ConditionalProbabilityTable(\n",
    "    [\n",
    "        #Lung? TB? \n",
    "        [\"T\",\"T\",\"T\",1],\n",
    "        [\"T\",\"T\",\"F\",0],\n",
    "        \n",
    "        [\"T\",\"F\",\"T\",1],\n",
    "        [\"T\",\"F\",\"F\",0],\n",
    "        \n",
    "        [\"F\",\"T\",\"T\",1],\n",
    "        [\"F\",\"T\",\"F\",0],\n",
    "        \n",
    "        [\"F\",\"F\",\"T\",0],\n",
    "        [\"F\",\"F\",\"F\",1]\n",
    "    ], [has_lung_cancer,has_tb])\n",
    "\n",
    "x_ray_abnormal = ConditionalProbabilityTable(\n",
    "    [\n",
    "        #TB or Cancer?\n",
    "        [\"T\",\"T\",0.98],\n",
    "        [\"T\",\"F\",0.02],\n",
    "        \n",
    "        [\"F\",\"T\",0.05],\n",
    "        [\"F\",\"F\",0.95]\n",
    "    ], [tb_or_cancer])\n",
    "\n",
    "dyspnea = ConditionalProbabilityTable(\n",
    "    [\n",
    "        #BC\n",
    "        [\"T\",\"T\",\"T\",0.9],\n",
    "        [\"T\",\"T\",\"F\",0.1],\n",
    "        \n",
    "        [\"T\",\"F\",\"T\",0.8],\n",
    "        [\"T\",\"F\",\"F\",0.2],\n",
    "        \n",
    "        [\"F\",\"T\",\"T\",0.7],\n",
    "        [\"F\",\"T\",\"F\",0.3],\n",
    "        \n",
    "        [\"F\",\"F\",\"T\",0.1],\n",
    "        [\"F\",\"F\",\"F\",0.9]\n",
    "    ], [has_bc, tb_or_cancer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next we have to create all the nodes\n",
    "asia_node = Node(visit_to_asia, name=\"asia\")\n",
    "tb_node = Node(has_tb, name=\"tb\")\n",
    "smoke_node = Node(smoke, name=\"smoke\")\n",
    "lung_node = Node(has_lung_cancer, name=\"lung\")\n",
    "bronc_node = Node(has_bc, name=\"bc\")\n",
    "either_node = Node(tb_or_cancer, name=\"either\")\n",
    "xray_node = Node(x_ray_abnormal,name=\"xray\")\n",
    "dysp_node = Node(dyspnea, name=\"dysp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we init the model\n",
    "model = BayesianNetwork(\"ASIA\")\n",
    "model.add_states(asia_node,\n",
    "                 tb_node,\n",
    "                 smoke_node,\n",
    "                 lung_node,\n",
    "                 bronc_node,\n",
    "                 either_node,\n",
    "                 xray_node,\n",
    "                 dysp_node)\n",
    "\n",
    "# Add all of the correct edges \n",
    "model.add_edge(asia_node, tb_node)\n",
    "\n",
    "model.add_edge(smoke_node, bronc_node)\n",
    "model.add_edge(smoke_node, lung_node)\n",
    "\n",
    "model.add_edge(tb_node,either_node)\n",
    "model.add_edge(lung_node,either_node)\n",
    "\n",
    "model.add_edge(either_node, xray_node)\n",
    "model.add_edge(either_node, dysp_node)\n",
    "\n",
    "model.add_edge(bronc_node, dysp_node)\n",
    "\n",
    "# And then commit our changes\n",
    "model.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to print the model structure\n",
    "def print_model_structure(model, features):\n",
    "    for i in range(len(features)):\n",
    "        parents = [features[pi] for pi in model.structure[i]]\n",
    "        print(f'Node \"{features[i]}\" has parents: {parents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll keep our features in this order for consistency\n",
    "features = [\n",
    "    \"Visit to Asia\",\n",
    "    \"Has TB\",\n",
    "    \"Smoker\",\n",
    "    \"Has Lung Cancer\",\n",
    "    \"Has Bronchitis\",\n",
    "    \"TB or Cancer\",\n",
    "    \"XRay Abnormal\",\n",
    "    \"Dyspnea\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make sure the structure of our newly created model is correct\n",
    "print_model_structure(model, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Predictions (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict` allows us to do inference based off of the data. It chooses the values that are the most likely.\n",
    "\n",
    "For example, let's say we have a patient who has an abnormal X-ray, but is not a smoker and hasn't visited Asia. We can then infer the most likely values for all of the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict([\n",
    "    [\"F\",None,\"F\",None,None,None,\"T\",None]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's say they *were* a smoker. See how it changes?\n",
    "model.predict([\n",
    "    [\"F\",None,\"T\",None,None,None,\"T\",None]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a little more detail and check out the actual probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_results(results):\n",
    "    for i,dist in enumerate(results):\n",
    "        print(features[i])\n",
    "        if isinstance(dist,str):\n",
    "            print(dist)\n",
    "        else:\n",
    "            print(dist.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = model.predict_proba([\n",
    "    [\"F\",None,\"F\",None,None,None,\"T\",None]\n",
    "])\n",
    "pretty_results(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Bayes net to calcualte the following probabilities:\n",
    "\n",
    "1. $P(xray=true | TBorCancer=true)$\n",
    "2. $P(xray=true | TBorCancer=true, TB=true)$\n",
    "3. $P(TB=true)$\n",
    "4. $P(TB=true | smoke=false)$\n",
    "5. $P(TB=true | smoke=false, TBorCancer=true)$\n",
    "\n",
    "What values are equivalent? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answers here:**\n",
    "\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "5.\n",
    "\n",
    "What values are equivalent? Why?\n",
    "\n",
    "And write code below to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As a reminder, here are the indices of the features\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can modify this code to help you\n",
    "pretty_results(model.predict_proba([\n",
    "    [None,None,'F',None,None,'T',None,None]\n",
    "])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Evaluating Bayes Nets (Follow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well this net is on inferencing from data. We're going to remove the Bronchitis column from this dataset, and see if our net can predict what the missing value should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some data we will use to generate our probabilities\n",
    "asia_data = pd.read_csv(\"Asia10k.csv\")\n",
    "asia_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asia_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make sure we're consistant with our labels\n",
    "asia_data = asia_data.replace(\"no\", \"F\").replace(\"yes\", \"T\")\n",
    "asia_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = asia_data.values.copy()\n",
    "indices = np.random.choice(asia_data.index, 1000)\n",
    "values = values[indices]\n",
    "values[:,4] = None\n",
    "values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(asia_data.values[indices,4],np.array(predictions)[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(asia_data.values[indices,4],np.array(predictions)[:,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5) Fitting a Bayes Net to Data (Group)\n",
    "\n",
    "In many applications, you may have a general idea of the structure of the Bayes Net, but do not have a list of probabilities. Luckily, given some data, we can fill out the probabilities in a given net. **Note:** You may only get similar results to the previous method, since it turns out this data was *simulated* from the given conditional probabilities. So, one would expect that the model would learn parameters like the ones we've given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fitted_model = model.fit(asia_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to print the probability distributions\n",
    "def print_distributions(model):\n",
    "    for i, state in enumerate(fitted_model.states):\n",
    "        print(features[i])\n",
    "        states = state.distribution.parameters[0]\n",
    "        if len(states)>1:\n",
    "            for state in states:\n",
    "                print(state)\n",
    "        #print(state.distribution.parameters[0])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_distributions(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a look at the learned probability distributions. Are they similar to \"expert\" ones given in the previous problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, perform the same evaluation that you did in the previous problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Bronchitis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did it perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6) Learning Structure from Data (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the most interesting problem is when we only have data, but we don't know the structure of the data (however, we still have a reason to believe that *it can be reperesented as Bayes Net*). Luckily, pomegranate has the ability to solve this problem as well. Given a dataset, we can use `from_samples` to build a Bayes net, structure and all, from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learned_model = BayesianNetwork.from_samples(asia_data, algorithm='exact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_model_structure(learned_model, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the model structure from experts vs learned from the data:**\n",
    "\n",
    "1. Draw out both models (the one you made earlier and the one learned) on a piece of paper.\n",
    "2. What are the differences you observe?\n",
    "3. Why might a model learned from the data have a different structure? Are some influences (edges) in the model more or less important than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7) BYOB: Build Your Own Bayes Net (Group, if time permits)\n",
    "If you find yourself with some extra time after this portion, consider **building a Bayes net that represents something in your daily life**. It could be the effect of traffic on a morning commute, deciding what to do for dinner, etc. It can be very small, only around 3-5 nodes probably (conditional probability tables are a pain!). Then play around with predictions and probabilities to see how various factors impact your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
